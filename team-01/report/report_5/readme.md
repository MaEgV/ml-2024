Интегрирован алгоритм Q-learning для поиска кратчайшего пути в лабиринте. Чтобы упростить задачу, вместо робота R2D2 используется сферический робот, поскольку управление R2D2 сложнее из-за специфики его передвижения.

В основе алгоритма лежит Q-таблица, размер которой соответствует размеру лабиринта. Для каждой ячейки таблицы создаётся список значений Q-функции, который содержит оценки для всех возможных направлений (вперёд, назад, влево, вправо). Сначала направления выбираются случайно с вероятностью ε (эпсилон), что позволяет роботу исследовать лабиринт и получать начальные данные для Q-таблицы. Однако с накоплением опыта и по мере заполнения таблицы робот всё чаще выбирает действия с максимальным значением Q, что позволяет ему принимать более осознанные решения.

Успешные траектории (то есть пути, ведущие робота к целевой точке) сохраняются в отдельном списке. Поскольку положительное вознаграждение даётся за достижение цели и посещение свободных клеток, использование суммы Q-значений не всегда объективно для выбора лучшего пути. Поэтому за оптимальную траекторию берётся та, которая содержит наименьшее количество шагов, чтобы обеспечить минимальное расстояние до цели.
